{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Posture Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python-headless<4.3 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (3.4.18.65)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from opencv-python-headless<4.3) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"opencv-python-headless<4.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_distance(point1, point2):\n",
        "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
        "\n",
        "def calculate_angle(point1, point2, point3):\n",
        "    \"\"\"Calculate angle between three points (in degrees).\n",
        "       When considering the points, only use the (x, y) coordinates\n",
        "    \"\"\"\n",
        "    a = np.array(point1)  # Convert point1 to a NumPy array\n",
        "    b = np.array(point2)  # Convert point2 to a NumPy array\n",
        "    c = np.array(point3)  # Convert point3 to a NumPy array\n",
        "\n",
        "    # Calculate vectors from point B to point A and from point B to point C\n",
        "    ba = a - b\n",
        "    bc = c - b\n",
        "\n",
        "    # Calculate the cosine of the angle between the vectors\n",
        "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "    \n",
        "    # Ensure cosine_angle is within -1 to 1 range to avoid NaN results\n",
        "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
        "\n",
        "    # Calculate the angle in radians and then convert to degrees\n",
        "    angle = np.arccos(cosine_angle)\n",
        "\n",
        "    return np.degrees(angle)\n",
        "    return np.degrees(angle)\n",
        "\n",
        "def normalize_score(angle, ideal_angle):\n",
        "    \"\"\"Normalize score between 0 and 1, where 0 is the ideal_angle.\"\"\"\n",
        "    deviation = abs(angle - ideal_angle)\n",
        "    normalized_score = deviation / ideal_angle\n",
        "    return min(normalized_score, 1)  # Ensure score does not exceed 1\n",
        "\n",
        "def heuristic_posture_scores(keypoints, FRAME_HEIGHT=1080):\n",
        "    \"\"\"Score the postural, spinal, and shoulder alignment.\n",
        "    Assuming keypoints: 0-Nose, 5-Left Shoulder, 6-Right Shoulder, 11-Left Hip, 12-Right Hip\"\"\"\n",
        "\n",
        "    # Calculate angles\n",
        "    back_angle = calculate_angle(keypoints[10:12], keypoints[22:24], keypoints[24:26])  # Shoulder to hips\n",
        "    neck_head_angle = calculate_angle(keypoints[0:2], keypoints[10:12], keypoints[12:14])  # Nose to shoulders\n",
        "\n",
        "    # Shoulder levelness: Ideal is a straight horizontal line, so difference in y-coordinates\n",
        "    shoulder_levelness = abs(keypoints[10] - keypoints[11])\n",
        "\n",
        "    # Assuming the maximum possible y-coordinate difference as the height of the frame (e.g., 1080 pixels),\n",
        "    # you may adjust this based on your actual frame height\n",
        "    shoulder_score = min(shoulder_levelness / FRAME_HEIGHT, 1)\n",
        "\n",
        "    # Normalize scores (0 is best, 1 is worst)\n",
        "    back_score = normalize_score(back_angle, 180)\n",
        "    neck_head_score = normalize_score(neck_head_angle, 180)\n",
        "\n",
        "    return {'back_score': back_score, 'neck_head_score': neck_head_score, 'shoulder_score': shoulder_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Train regression model on real data points from yoga_train_data.csv\n",
        "\n",
        "df = pd.read_csv('/Users/jennycai/Desktop/TreeHacks2024/ml/sample_data/yoga_train_data.csv') # TODO: CHANGE TO RELATIVE PATH\n",
        "cols_of_interest = [\n",
        "    'nose_x', 'nose_y', 'nose_score',\n",
        "    'left_eye_x', 'left_eye_y', 'left_eye_score',\n",
        "    'right_eye_x', 'right_eye_y', 'right_eye_score',\n",
        "    'left_ear_x', 'left_ear_y', 'left_ear_score',\n",
        "    'right_ear_x', 'right_ear_y', 'right_ear_score',\n",
        "    'left_shoulder_x', 'left_shoulder_y', 'left_shoulder_score',\n",
        "    'right_shoulder_x', 'right_shoulder_y', 'right_shoulder_score',\n",
        "    'left_elbow_x', 'left_elbow_y', 'left_elbow_score',\n",
        "    'right_elbow_x', 'right_elbow_y', 'right_elbow_score',\n",
        "    'left_wrist_x', 'left_wrist_y', 'left_wrist_score',\n",
        "    'right_wrist_x', 'right_wrist_y', 'right_wrist_score',\n",
        "    'left_hip_x', 'left_hip_y', 'left_hip_score',\n",
        "    'right_hip_x', 'right_hip_y', 'right_hip_score',\n",
        "    'left_knee_x', 'left_knee_y', 'left_knee_score',\n",
        "    'right_knee_x', 'right_knee_y', 'right_knee_score',\n",
        "    'left_ankle_x', 'left_ankle_y', 'left_ankle_score',\n",
        "    'right_ankle_x', 'right_ankle_y', 'right_ankle_score'\n",
        "]\n",
        "cols_of_interest = [col for col in cols_of_interest if not col.endswith('_score')]\n",
        "\n",
        "df = df.filter(cols_of_interest)\n",
        "X = df.to_numpy() # (968, 34)\n",
        "num_samples = X.shape[0]\n",
        "\n",
        "y_back, y_shoulder, y_neck_head = [], [], []\n",
        "# Simulate scores for back, shoulder, and neck/head alignment (ranging from 0 to 1)\n",
        "for keypoints in X:\n",
        "  curr_y_back, curr_y_neck_head, curr_y_shoulder = heuristic_posture_scores(keypoints).values()\n",
        "  y_back.append(curr_y_back)\n",
        "  y_shoulder.append(curr_y_shoulder)\n",
        "  y_neck_head.append(curr_y_neck_head)\n",
        "\n",
        "y_back, y_shoulder, y_neck_head = np.array(y_back).reshape((num_samples, 1)), np.array(y_shoulder).reshape((num_samples, 1)), np.array(y_neck_head).reshape((num_samples, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train_back, y_test_back = train_test_split(X, y_back, test_size=0.2, random_state=42)\n",
        "_, _, y_train_shoulder, y_test_shoulder = train_test_split(X, y_shoulder, test_size=0.2, random_state=42)\n",
        "_, _, y_train_neck_head, y_test_neck_head = train_test_split(X, y_neck_head, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train separate models for each score\n",
        "model_back = LinearRegression().fit(X_train, y_train_back)\n",
        "model_shoulder = LinearRegression().fit(X_train, y_train_shoulder)\n",
        "model_neck_head = LinearRegression().fit(X_train, y_train_neck_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Movenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85hIUtxSFDyN",
        "outputId": "d6da5f7a-aa95-4118-9ffd-450e16be881d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /Users/jennycai/anaconda3/lib/python3.11/site-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
            "Requirement already satisfied: mediapipe in /Users/jennycai/anaconda3/lib/python3.11/site-packages (0.9.1.0)\n",
            "Requirement already satisfied: absl-py in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (2.1.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (22.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (3.7.2)\n",
            "Requirement already satisfied: numpy in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (1.24.3)\n",
            "Requirement already satisfied: opencv-contrib-python in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (4.9.0.80)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: tensorflow in /Users/jennycai/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub in /Users/jennycai/anaconda3/lib/python3.11/site-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow-hub) (1.24.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /Users/jennycai/anaconda3/lib/python3.11/site-packages (from tensorflow-hub) (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "!pip install mediapipe\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GHFROF6OFyIJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-17 14:14:41.006691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#Computer vision/graphics library\n",
        "import cv2\n",
        "\n",
        "# Gif writer\n",
        "import imageio\n",
        "\n",
        "# Display libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Calculations and Deep Learning library\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05LyiG2Gd2p"
      },
      "source": [
        "### Set up colors + size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qhQncwy5GSBq"
      },
      "outputs": [],
      "source": [
        "blue = (173, 216, 230)\n",
        "green = (144, 238, 144)\n",
        "\n",
        "EDGE_COLORS = {\n",
        "    (0, 1): green,\n",
        "    (0, 2): blue,\n",
        "    (1, 3): green,\n",
        "    (2, 4): blue,\n",
        "    (0, 5): green,\n",
        "    (0, 6): blue,\n",
        "    (5, 7): green,\n",
        "    (7, 9): blue,\n",
        "    (6, 8): green,\n",
        "    (8, 10): blue,\n",
        "    (5, 6): green,\n",
        "    (5, 11): blue,\n",
        "    (6, 12): green,\n",
        "    (11, 12): blue,\n",
        "    (11, 13): green,\n",
        "    (13, 15): blue,\n",
        "    (12, 14): green,\n",
        "    (14, 16): blue\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HWxExtkkGpOR"
      },
      "outputs": [],
      "source": [
        "#initial_width, initial_height = (461,250)\n",
        "WIDTH = HEIGHT = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O64eM_oGiHA"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tD93KQ53Gkq-"
      },
      "outputs": [],
      "source": [
        "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
        "movenet = model.signatures[\"serving_default\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVbK6IjfG49u"
      },
      "source": [
        "### Define the loop\n",
        "Steps : loop through the results ---> Draw the keypoints ----> Draw the edges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KjgDuLMvGyOz"
      },
      "outputs": [],
      "source": [
        "def loop(frame, keypoints, threshold=0.11):\n",
        "    \"\"\"\n",
        "    Main loop : Draws the keypoints and edges for each instance\n",
        "    \"\"\"\n",
        "\n",
        "    # Loop through the results\n",
        "    for instance in keypoints:\n",
        "        # Draw the keypoints and get the denormalized coordinates\n",
        "        denormalized_coordinates = draw_keypoints(frame, instance, threshold)\n",
        "        # Draw the edges\n",
        "        draw_edges(denormalized_coordinates, frame, EDGE_COLORS, threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V0wce5nG_y3"
      },
      "source": [
        "### Draw points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "orXk_5zcHBiI"
      },
      "outputs": [],
      "source": [
        "def draw_keypoints(frame, keypoints, threshold=0.11):\n",
        "    \"\"\"Draws the keypoints on a image frame\"\"\"\n",
        "\n",
        "    # Denormalize the coordinates : multiply the normalized coordinates by the input_size(width,height)\n",
        "    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n",
        "    #Iterate through the points\n",
        "    for keypoint in denormalized_coordinates:\n",
        "        # Unpack the keypoint values : y, x, confidence score\n",
        "        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n",
        "        if keypoint_confidence > threshold:\n",
        "            \"\"\"\"\n",
        "            Draw the circle\n",
        "            Note : A thickness of -1 px will fill the circle shape by the specified color.\n",
        "            \"\"\"\n",
        "            cv2.circle(\n",
        "                img=frame,\n",
        "                center=(int(keypoint_x), int(keypoint_y)),\n",
        "                radius=4,\n",
        "                color=(255,0,0),\n",
        "                thickness=-1\n",
        "            )\n",
        "    return denormalized_coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe9CaGsnHF7D"
      },
      "source": [
        "### Draw edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_U7_solgHHTM"
      },
      "outputs": [],
      "source": [
        "def draw_edges(denormalized_coordinates, frame, edges_colors, threshold=0.11):\n",
        "    \"\"\"\n",
        "    Draws the edges on a image frame\n",
        "    \"\"\"\n",
        "\n",
        "    # Iterate through the edges\n",
        "    for edge, color in edges_colors.items():\n",
        "        # Get the dict value associated to the actual edge\n",
        "        p1, p2 = edge\n",
        "        # Get the points\n",
        "        y1, x1, confidence_1 = denormalized_coordinates[p1]\n",
        "        y2, x2, confidence_2 = denormalized_coordinates[p2]\n",
        "        # Draw the line from point 1 to point 2, the confidence > threshold\n",
        "        if (confidence_1 > threshold) & (confidence_2 > threshold):\n",
        "            cv2.line(\n",
        "                img=frame,\n",
        "                pt1=(int(x1), int(y1)),\n",
        "                pt2=(int(x2), int(y2)),\n",
        "                color=color,\n",
        "                thickness=2,\n",
        "                lineType=cv2.LINE_AA # Gives anti-aliased (smoothed) line which looks great for curves\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03rLl0g8HQP8"
      },
      "source": [
        "### Process real-time frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bKcbylECHSzW"
      },
      "outputs": [],
      "source": [
        "def initialize_webcam():\n",
        "    \"\"\"\n",
        "    Initializes webcam for capturing and returns necessary objects for processing.\n",
        "    \"\"\"\n",
        "    # Initialize webcam capture; 0 usually refers to the default webcam.\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open webcam.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Assuming frame count is not fixed for a continuous stream\n",
        "    frame_count = None\n",
        "    \n",
        "    # Initialize an empty list to hold processed frames (if needed)\n",
        "    output_frames = []\n",
        "\n",
        "    # Get initial shape (width, height) from the first frame to setup processing parameters\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Could not read frame from webcam.\")\n",
        "        cap.release()\n",
        "        return None, None, None, None\n",
        "    \n",
        "    initial_shape = [frame.shape[1], frame.shape[0]]  # Width, Height\n",
        "\n",
        "    # Return to the beginning of the stream\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    \n",
        "    return cap, frame_count, output_frames, initial_shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSDygRfeHWtG"
      },
      "source": [
        "### Run inference\n",
        "\n",
        "Connects to the webcam, and displays your live skeleton estimate alongside a real-time posture scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftfgwNXxHYMt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import imageio\n",
        "from IPython.display import display, Image  # Assuming this is run in a Jupyter Notebook or similar environment\n",
        "import time\n",
        "\n",
        "def process_webcam_input(movenet, WIDTH=192, HEIGHT=192, MAX_FRAMES=1000):\n",
        "    \"\"\"\n",
        "    Continuously captures frames from the webcam, displays the estimated skeleton in real-time.\n",
        "    \"\"\"\n",
        "    cap, frame_count, output_frames, initial_shape = initialize_webcam()\n",
        "    if cap is None:\n",
        "        print(\"Webcam not accessible.\")\n",
        "        return\n",
        "\n",
        "    output_frames = []\n",
        "\n",
        "    print(\"Starting real-time inference...\")\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Failed to capture frame.\")\n",
        "            break\n",
        "\n",
        "        # Process frame\n",
        "        image = cv2.resize(frame, (WIDTH, HEIGHT))\n",
        "        input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n",
        "        input_image = tf.expand_dims(input_image, axis=0)\n",
        "\n",
        "        # Run inference\n",
        "        results = movenet(input_image)\n",
        "        \"\"\"\n",
        "        Output shape :  [1, 6, 56] ---> (batch size), (instances), (xy keypoints coordinates and score from [0:50]\n",
        "        and [ymin, xmin, ymax, xmax, score] for the remaining elements)\n",
        "        First, let's resize it to a more convenient shape, following this logic :\n",
        "        - First channel ---> each instance\n",
        "        - Second channel ---> 17 keypoints for each instance\n",
        "        - The 51st values of the last channel ----> the confidence score.\n",
        "        Thus, the Tensor is reshaped without losing important information.\n",
        "        \"\"\"\n",
        "        keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3))\n",
        "\n",
        "        loop(image, keypoints, threshold=0.11)\n",
        "\n",
        "        # Get the output frame : reshape to the original size\n",
        "        frame_rgb = cv2.cvtColor(\n",
        "            cv2.resize(\n",
        "                image,(initial_shape[0], initial_shape[1]),\n",
        "                interpolation=cv2.INTER_LANCZOS4\n",
        "            ),\n",
        "            cv2.COLOR_BGR2RGB # OpenCV processes BGR images instead of RGB\n",
        "        )\n",
        "        \n",
        "        # REAL-TIME OUTPUTS\n",
        "        \n",
        "        # skeleton frames overlayed on webcame output\n",
        "        cv2.imshow('Skeleton', frame_rgb)\n",
        "        \n",
        "        # posture scores printed to console\n",
        "        keypoints_yx = keypoints[0, :, :2] # Select the first batch of keypoints and then take only the (y, x) coordinates, discarding the confidence scores\n",
        "        \n",
        "        # Flatten the array to have a shape of (1, 34)\n",
        "        keypoints_processed = keypoints_yx.flatten().reshape(1, -1)\n",
        "        \n",
        "        score_back = model_back.predict(keypoints_processed)\n",
        "        score_shoulder = model_shoulder.predict(keypoints_processed)\n",
        "        score_neck_head = model_neck_head.predict(keypoints_processed)\n",
        "\n",
        "        print(f\"Predicted Back Alignment Score: {score_back[0][0]:.2f}\")\n",
        "        print(f\"Predicted Shoulder Alignment Score: {score_shoulder[0][0]:.2f}\")\n",
        "        print(f\"Predicted Neck/Head Alignment Score: {score_neck_head[0][0]:.2f}\")\n",
        "        \n",
        "        \n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Example usage, assuming 'movenet' is your loaded TensorFlow model\n",
        "process_webcam_input(movenet)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
